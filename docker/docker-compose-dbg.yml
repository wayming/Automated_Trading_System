services:
  ivscraper:
    build:
      context: ../  # Set build context to project root
      dockerfile: docker/Dockerfile.scraper
    container_name: python-scraper-iv
    volumes:
      - ../output:/app/output  # Mount the current directory to /app in the container
      - /tmp/.X11-unix:/tmp/.X11-unix # Show chrome browser
    environment:
      - DISPLAY=$DISPLAY  # Show chrome browser
    networks:
      - scraper-network
    restart: always  # Restart container if it crashes
    command: python -m news_scraper.scraper_investing
    depends_on:
      rabbitmq:
        condition: service_healthy # RabbitMQ begin to accept connections

  selenium-hub:
    image: selenium/hub:4.35.0
    container_name: selenium-hub
    ports:
      - "4444:4444"
    environment:
      - SE_EVENT_BUS_HOST=selenium-hub
      - SE_EVENT_BUS_PUBLISH_PORT=4442
      - SE_EVENT_BUS_SUBSCRIBE_PORT=4443
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:4444/wd/hub/status"]
      interval: 5s
      timeout: 3s
      retries: 10
    networks:
      - scraper-network

  chrome-node:
    image: selenium/node-chrome:4.35.0
    container_name: selenium-node-chrome
    depends_on:
      - selenium-hub
    environment:
      - SE_EVENT_BUS_HOST=selenium-hub
      - SE_EVENT_BUS_PUBLISH_PORT=4442
      - SE_EVENT_BUS_SUBSCRIBE_PORT=4443
      - SE_NODE_GRID_URL=http://selenium-hub:4444
      - SE_NODE_MAX_SESSIONS=1
      - SE_NODE_OPTS="--no-sandbox --disable-gpu --headless=new --disable-dev-shm-usage --window-size=1280,1024"
    shm_size: "2g"
    volumes:
      - /dev/shm:/dev/shm
    networks:
      - scraper-network

  tvscraper:
    build:
      context: ../  # Set build context to project root
      dockerfile: docker/Dockerfile.scraper
    container_name: python-scraper-tv
    volumes:
      - ../output:/app/output  # Mount the current directory to /app in the container
      # - /tmp/.X11-unix:/tmp/.X11-unix # Show chrome browser
    environment:
      # - DISPLAY=$DISPLAY  # Show chrome browser
      - RABBITMQ_HOST=rabbitmq
      - TRADE_VIEW_USER=${TRADE_VIEW_USER}
      - TRADE_VIEW_PASS=${TRADE_VIEW_PASS}
      - SELENIUM_HUB_URL=http://selenium-hub:4444/wd/hub
    ports:
      - "9222:9222"
    networks:
      - scraper-network
    restart: always  # Restart container if it crashes
    command: python -m news_scraper.scraper_trading_view
    depends_on:
      selenium-hub:
        condition: service_healthy
      chrome-node:
        condition: service_started
      rabbitmq:
        condition: service_healthy # RabbitMQ begin to accept connections

  rabbitmq:
    image: rabbitmq:3-management
    ports:
      - "5672:5672"
      - "15672:15672"  # Management UI
    restart: always
    # environment:
    #   - RABBITMQ_SERVER_ADDITIONAL_ERL_ARGS=-rabbit heartbeat 600 channel_operation_timeout 600000 log_levels [{connection,debug},{channel,debug}]
    volumes:
      - ../rabbitmq/rabbitmq_data:/var/lib/rabbitmq
      - ../rabbitmq/rabbitmq_logs:/var/log/rabbitmq
    environment:
      - RABBITMQ_DEFAULT_USER=admin
      - RABBITMQ_DEFAULT_PASS=password
    networks:
      - scraper-network
    healthcheck:
      test: ["CMD", "rabbitmq-diagnostics", "check_running"]
      interval: 5s
      timeout: 5s
      retries: 10

  ivanalyser:
    build:
      context: ../  # Set build context to project root
      dockerfile: docker/Dockerfile.analyser
    environment:
      - AWS_GATEWAY_ENDPOINT=aws_gateway:50053
      - DEEPSEEK_API_KEY=${DEEPSEEK_API_KEY}
    volumes:
      - ../output:/app/output  # Mount the current directory to /app in the container
    command: python -m news_scraper.analyser_investing
    depends_on:
      rabbitmq:
        condition: service_healthy # RabbitMQ begin to accept connections
      mock_executor:
        condition: service_started
    networks:
      - scraper-network

  tvanalyser:
    build:
      context: ../  # Set build context to project root
      dockerfile: docker/Dockerfile.analyser
    environment:
      - AWS_GATEWAY_ENDPOINT=aws_gateway:50053
      - DEEPSEEK_API_KEY=${DEEPSEEK_API_KEY}
    volumes:
      - ../output:/app/output  # Mount volume
    command: python -m news_scraper.analyser_trading_view
    depends_on:
      rabbitmq:
        condition: service_healthy # RabbitMQ begin to accept connections
      mock_executor:
        condition: service_started
      aws_gateway:
        condition: service_started
    networks:
      - scraper-network

  mock_executor:
    build:
      context: ../  # Set build context to project root
      dockerfile: docker/Dockerfile.mock.executor
    ports:
      - "50051:50051"
    volumes:
      - ../output:/app/output  # Mount volume
    depends_on:
      stock_hub:
        condition: service_started
    networks:
      - scraper-network

  stock_hub:
    build:
      context: ../  # Set build context to project root
      dockerfile: docker/Dockerfile.stock.hub
    ports:
      - "50052:50052"
    volumes:
      - ../output:/app/output  # Mount volume
    networks:
      - scraper-network

  aws_gateway:
    build:
      context: ../  # Set build context to project root
      dockerfile: docker/Dockerfile.aws.gateway
    ports:
      - "50053:50053"
    volumes:
      - ../output:/app/output  # Mount volume
    networks:
      - scraper-network

networks:
  scraper-network:
    driver: bridge
