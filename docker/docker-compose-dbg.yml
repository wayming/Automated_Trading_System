services:

  selenium-hub:
    image: selenium/hub:4.35.0
    container_name: selenium-hub
    ports:
      - "4444:4444"
    environment:
      - SE_EVENT_BUS_HOST=selenium-hub
      - SE_EVENT_BUS_PUBLISH_PORT=4442
      - SE_EVENT_BUS_SUBSCRIBE_PORT=4443
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:4444/wd/hub/status"]
      interval: 5s
      timeout: 3s
      retries: 10
    networks:
      - scraper-network

  chrome-node:
    image: selenium/node-chrome:4.35.0
    container_name: selenium-node-chrome
    depends_on:
      - selenium-hub
    environment:
      - SE_EVENT_BUS_HOST=selenium-hub
      - SE_EVENT_BUS_PUBLISH_PORT=4442
      - SE_EVENT_BUS_SUBSCRIBE_PORT=4443
      - SE_NODE_GRID_URL=http://selenium-hub:4444
      - SE_NODE_MAX_SESSIONS=1
      - SE_NODE_OPTS="--no-sandbox --disable-gpu --headless=new --disable-dev-shm-usage --window-size=1280,1024"
    shm_size: "2g"
    volumes:
      - /dev/shm:/dev/shm
    networks:
      - scraper-network

  rabbitmq:
    build:
      context: ../  # Set build context to project root
      dockerfile: docker/Dockerfile.rabbitmq
    ports:
      - "5672:5672"
      - "15672:15672"  # Management UI
      - "15692:15692"  # Prometheus metrics
    restart: always
    # environment:
    #   - RABBITMQ_SERVER_ADDITIONAL_ERL_ARGS=-rabbit heartbeat 600 channel_operation_timeout 600000 log_levels [{connection,debug},{channel,debug}]
    volumes:
      - ../rabbitmq/rabbitmq_data:/var/lib/rabbitmq
      - ../rabbitmq/rabbitmq_logs:/var/log/rabbitmq
    environment:
      - RABBITMQ_DEFAULT_USER=admin
      - RABBITMQ_DEFAULT_PASS=password
    networks:
      - scraper-network
    healthcheck:
      test: ["CMD", "rabbitmq-diagnostics", "check_running"]
      interval: 5s
      timeout: 5s
      retries: 10


  tvscraper:
    build:
      context: ../  # Set build context to project root
      dockerfile: docker/Dockerfile.scraper
    container_name: python-scraper-tv
    volumes:
      - ../output:/app/output  # Mount the current directory to /app in the container
      # - /tmp/.X11-unix:/tmp/.X11-unix # Show chrome browser
    environment:
      # - DISPLAY=$DISPLAY  # Show chrome browser
      - RABBITMQ_HOST=rabbitmq
      - TRADE_VIEW_USER=${TRADE_VIEW_USER}
      - TRADE_VIEW_PASS=${TRADE_VIEW_PASS}
      - SELENIUM_HUB_URL=http://selenium-hub:4444/wd/hub
    ports:
      - "9222:9222"
    networks:
      - scraper-network
    restart: always  # Restart container if it crashes
    command: python -m news_scraper.scraper_trading_view
    depends_on:
      selenium-hub:
        condition: service_healthy
      chrome-node:
        condition: service_started
      rabbitmq:
        condition: service_healthy # RabbitMQ begin to accept connections

  ivscraper:
    build:
      context: ../  # Set build context to project root
      dockerfile: docker/Dockerfile.scraper
    container_name: python-scraper-iv
    volumes:
      - ../output:/app/output  # Mount the current directory to /app in the container
      - /tmp/.X11-unix:/tmp/.X11-unix # Show chrome browser
    environment:
      - DISPLAY=$DISPLAY  # Show chrome browser
    networks:
      - scraper-network
    restart: always  # Restart container if it crashes
    command: python -m news_scraper.scraper_investing
    depends_on:
      rabbitmq:
        condition: service_healthy # RabbitMQ begin to accept connections

  deepseek_analyser:
    build:
      context: ../  # Set build context to project root
      dockerfile: docker/Dockerfile.analyser
    environment:
      - AWS_GATEWAY_ENDPOINT=aws_gateway:50053
      - DEEPSEEK_API_KEY=${DEEPSEEK_API_KEY}
    volumes:
      - ../output:/app/output  # Mount the current directory to /app in the container
    command: python -m news_analyser.deepseek_analyser
    depends_on:
      rabbitmq:
        condition: service_healthy # RabbitMQ begin to accept connections
      mock_executor:
        condition: service_started
      aws_gateway:
        condition: service_started
    networks:
      - scraper-network

  mock_executor:
    build:
      context: ../  # Set build context to project root
      dockerfile: docker/Dockerfile.mock.executor
    ports:
      - "50051:50051"
    volumes:
      - ../output:/app/output  # Mount volume
    depends_on:
      stock_hub:
        condition: service_started
    networks:
      - scraper-network

  stock_hub:
    build:
      context: ../  # Set build context to project root
      dockerfile: docker/Dockerfile.stock.hub
    ports:
      - "50052:50052"
    volumes:
      - ../output:/app/output  # Mount volume
    networks:
      - scraper-network

  aws_gateway:
    build:
      context: ../  # Set build context to project root
      dockerfile: docker/Dockerfile.aws.gateway
    ports:
      - "50053:50053"
    environment:
      - HTTP_API_ENDPOINT=${HTTP_API_ENDPOINT}
    volumes:
      - ../output:/app/output  # Mount volume
    networks:
      - scraper-network

  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    volumes:
      - ../monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
    ports:
      - "9090:9090"
    networks:
      - scraper-network
    restart: unless-stopped

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3001:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
    networks:
      - scraper-network
    depends_on:
      prometheus:
        condition: service_started
    restart: unless-stopped

networks:
  scraper-network:
    driver: bridge
