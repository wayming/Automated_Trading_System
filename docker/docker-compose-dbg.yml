version: "3.8"
services:
  ivscraper:
    build:
      context: ../  # Set build context to project root
      dockerfile: docker/Dockerfile.scraper
    container_name: python-scraper-iv
    volumes:
      - ../output:/app/output  # Mount the current directory to /app in the container
      - /tmp/.X11-unix:/tmp/.X11-unix # Show chrome browser
    environment:
      - DISPLAY=$DISPLAY  # Show chrome browser
    env_file:
      - .env
    networks:
      - scraper-network
    restart: always  # Restart container if it crashes
    command: python -m news_scraper.scraper_investing
    depends_on:
      rabbitmq:
        condition: service_healthy # RabbitMQ begin to accept connections

  tvscraper:
    build:
      context: ../  # Set build context to project root
      dockerfile: docker/Dockerfile.scraper
    container_name: python-scraper-tv
    volumes:
      - ../output:/app/output  # Mount the current directory to /app in the container
      - /tmp/.X11-unix:/tmp/.X11-unix # Show chrome browser
    environment:
      - DISPLAY=$DISPLAY  # Show chrome browser
    env_file:
      - .env
    networks:
      - scraper-network
    restart: always  # Restart container if it crashes
    command: python -m news_scraper.scraper_trading_view
    depends_on:
      rabbitmq:
        condition: service_healthy # RabbitMQ begin to accept connections

  rabbitmq:
    image: rabbitmq:3-management
    ports:
      - "5672:5672"
      - "15672:15672"  # Management UI
    restart: always
    # environment:
    #   - RABBITMQ_SERVER_ADDITIONAL_ERL_ARGS=-rabbit heartbeat 600 channel_operation_timeout 600000 log_levels [{connection,debug},{channel,debug}]
    volumes:
      - ../rabbitmq/rabbitmq_data:/var/lib/rabbitmq
      - ../rabbitmq/rabbitmq_logs:/var/log/rabbitmq
    networks:
      - scraper-network
    healthcheck:
      test: ["CMD", "rabbitmq-diagnostics", "check_running"]
      interval: 5s
      timeout: 5s
      retries: 10

  ivanalyser:
    build:
      context: ../  # Set build context to project root
      dockerfile: docker/Dockerfile.analyser
    env_file:
      - .env
    environment:
      - AWS_GATEWAY_ENDPOINT=aws_gateway:50053
    volumes:
      - ../output:/app/output  # Mount the current directory to /app in the container
    command: python -m news_scraper.analyser_investing
    depends_on:
      rabbitmq:
        condition: service_healthy # RabbitMQ begin to accept connections
      mock_executor:
        condition: service_started
    networks:
      - scraper-network

  tvanalyser:
    build:
      context: ../  # Set build context to project root
      dockerfile: docker/Dockerfile.analyser
    env_file:
      - .env
    environment:
      - AWS_GATEWAY_ENDPOINT=aws_gateway:50053
    volumes:
      - ../output:/app/output  # Mount volume
    command: python -m news_scraper.analyser_trading_view
    depends_on:
      rabbitmq:
        condition: service_healthy # RabbitMQ begin to accept connections
      mock_executor:
        condition: service_started
      aws_gateway:
        condition: service_started
    networks:
      - scraper-network

  mock_executor:
    build:
      context: ../  # Set build context to project root
      dockerfile: docker/Dockerfile.mock.executor
    ports:
      - "50051:50051"
    volumes:
      - ../output:/app/output  # Mount volume
    depends_on:
      stock_hub:
        condition: service_started
    networks:
      - scraper-network

  stock_hub:
    build:
      context: ../  # Set build context to project root
      dockerfile: docker/Dockerfile.stock.hub
    ports:
      - "50052:50052"
    volumes:
      - ../output:/app/output  # Mount volume
    networks:
      - scraper-network

  aws_gateway:
    build:
      context: ../  # Set build context to project root
      dockerfile: docker/Dockerfile.aws.gateway
    env_file:
      - .env
    ports:
      - "50053:50053"
    volumes:
      - ../output:/app/output  # Mount volume
    networks:
      - scraper-network

networks:
  scraper-network:
    driver: bridge
